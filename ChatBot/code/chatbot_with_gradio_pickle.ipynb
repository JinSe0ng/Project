{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pickle5\n",
      "  Downloading pickle5-0.0.11.tar.gz (132 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: pickle5\n",
      "  Building wheel for pickle5 (pyproject.toml): started\n",
      "  Building wheel for pickle5 (pyproject.toml): finished with status 'error'\n",
      "Failed to build pickle5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~angsmith (c:\\Users\\seul\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-hGK6LnUe-py3.11\\Lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Ã— Building wheel for pickle5 (pyproject.toml) did not run successfully.\n",
      "  â”‚ exit code: 1\n",
      "  â•°â”€> [15 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\\lib.win-amd64-cpython-311\\pickle5\n",
      "      copying pickle5\\pickle.py -> build\\lib.win-amd64-cpython-311\\pickle5\n",
      "      copying pickle5\\pickletools.py -> build\\lib.win-amd64-cpython-311\\pickle5\n",
      "      copying pickle5\\__init__.py -> build\\lib.win-amd64-cpython-311\\pickle5\n",
      "      creating build\\lib.win-amd64-cpython-311\\pickle5\\test\n",
      "      copying pickle5\\test\\pickletester.py -> build\\lib.win-amd64-cpython-311\\pickle5\\test\n",
      "      copying pickle5\\test\\test_pickle.py -> build\\lib.win-amd64-cpython-311\\pickle5\\test\n",
      "      copying pickle5\\test\\test_picklebuffer.py -> build\\lib.win-amd64-cpython-311\\pickle5\\test\n",
      "      copying pickle5\\test\\__init__.py -> build\\lib.win-amd64-cpython-311\\pickle5\\test\n",
      "      running build_ext\n",
      "      building 'pickle5._pickle' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for pickle5\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pickle5)\n"
     ]
    }
   ],
   "source": [
    "pip install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seul\\AppData\\Local\\Temp\\ipykernel_13508\\2145101619.py:172: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n",
      "C:\\Users\\seul\\AppData\\Local\\Temp\\ipykernel_13508\\2145101619.py:179: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(temperature=0.2)\n",
      "C:\\Users\\seul\\AppData\\Local\\Temp\\ipykernel_13508\\2145101619.py:194: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
      "c:\\Users\\seul\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-hGK6LnUe-py3.11\\Lib\\site-packages\\gradio\\components\\chatbot.py:284: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seul\\AppData\\Local\\Temp\\ipykernel_13508\\2145101619.py:208: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = retrieval_chain({\"question\": question, \"chat_history\": chat_history})\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import Document as LDocument\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import pymysql\n",
    "import locale\n",
    "import gradio as gr\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API ì„¤ì •\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "import pickle\n",
    "# í”¼í´ íŒŒì¼ì„ ì €ì¥í•˜ëŠ” í•¨ìˆ˜\n",
    "def save_to_pickle(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "# í”¼í´ íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜\n",
    "def load_from_pickle(filename):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# MySQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•¨ìˆ˜ (ì˜ˆì‹œ)\n",
    "def connect_to_db():\n",
    "    return pymysql.connect(\n",
    "        host='database-1.cxcqeqcc6xxo.ap-northeast-2.rds.amazonaws.com',\n",
    "        port=3306,\n",
    "        user='admin',\n",
    "        passwd='sesaclangchain',\n",
    "        db='sesaclangchain',\n",
    "        charset=\"utf8mb4\"\n",
    "    )\n",
    "\n",
    "# ì˜¨ë¼ì¸ ê°•ì˜ì™€ ì˜¤í”„ë¼ì¸ ê°•ì˜ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
    "def fetch_courses():\n",
    "    connection = connect_to_db()\n",
    "    cursor = connection.cursor(pymysql.cursors.DictCursor)\n",
    "\n",
    "    # ì˜¨ë¼ì¸ ê°•ì˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "    online_query = \"SELECT * FROM online_course_tbl\"\n",
    "    cursor.execute(online_query)\n",
    "    online_courses = cursor.fetchall()\n",
    "\n",
    "    # ì˜¤í”„ë¼ì¸ ê°•ì˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "    offline_query = \"SELECT * FROM offline_course_tbl\"\n",
    "    cursor.execute(offline_query)\n",
    "    offline_courses = cursor.fetchall()\n",
    "\n",
    "    # ê³µì§€ \n",
    "    notice_query = \"SELECT * FROM notice_tbl\"\n",
    "    cursor.execute(notice_query)\n",
    "    notice_info = cursor.fetchall()\n",
    "\n",
    "    # qna \n",
    "    qna_query = 'SELECT * FROM qna_tbl'\n",
    "    cursor.execute(qna_query)\n",
    "    qna_info = cursor.fetchall()\n",
    "\n",
    "    # ë¦¬ë·°\n",
    "    review_query = 'select * from review'\n",
    "    cursor.execute(review_query)\n",
    "    review_info = cursor.fetchall()\n",
    "\n",
    "    # ê°•ì˜ ì´ë¯¸ì§€\n",
    "    course_image_query = 'select * from class_img2txt'\n",
    "    cursor.execute(course_image_query)\n",
    "    course_image_info = cursor.fetchall()\n",
    "\n",
    "    connection.close()\n",
    "\n",
    "    return online_courses, offline_courses, notice_info, qna_info, review_info,course_image_info\n",
    "\n",
    "# ê°•ì˜ ì •ë³´ë¥¼ ë¬¸ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "# í…ìŠ¤íŠ¸ë¥¼ ì¼ì •í•œ ê¸¸ì´ë¡œ ì²­í¬ ë¶„í• í•˜ëŠ” í•¨ìˆ˜\n",
    "def chunk_text(text, chunk_size=500, chunk_overlap=50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return text_splitter.split_text(text) if len(text) > chunk_size else [text]  # ë„ˆë¬´ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜\n",
    "\n",
    "# ë°ì´í„°ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "def convert_to_documents(online_courses, offline_courses, notice_info, qna_info, review_info, course_image_info):\n",
    "    documents = []\n",
    "\n",
    "    # ë°ì´í„°ë¥¼ ë°˜ë³µí•´ì„œ ì²˜ë¦¬í•˜ëŠ” ê³µí†µ í•¨ìˆ˜\n",
    "    def process_and_chunk(data_list, format_func):\n",
    "        for item in data_list:\n",
    "            doc_text = format_func(item)\n",
    "            chunks = chunk_text(doc_text, chunk_size=500)\n",
    "            for chunk in chunks:\n",
    "                documents.append(LDocument(page_content=chunk))\n",
    "\n",
    "    # ê° ë°ì´í„°ì— ë§ëŠ” í…ìŠ¤íŠ¸ í¬ë§·íŒ…\n",
    "    process_and_chunk(online_courses, lambda course: (\n",
    "        f\"ê°•ì˜ ì œëª©: {course['course_title']}\\n\"\n",
    "        f\"ê°•ì˜ ë¶„ë¥˜: {course['course_category']}\\n\"\n",
    "        f\"ê°•ì˜ ë‚œë„: {course['course_difficulty']}\\n\"\n",
    "        f\"êµìœ¡ ëª©í‘œ: {course['course_objective']}\\n\"\n",
    "        f\"ìˆ˜ê°• ëŒ€ìƒ: {course['target_audience']}\\n\"\n",
    "        f\"ìˆ˜ê°• ì‹œê°„: {course['class_duration']}\\n\"\n",
    "        f\"ìˆ˜ê°• ê¸°ê°„: {course['class_period']}\\n\"\n",
    "        f\"ì§€ì› ê¸°ê°„: {course['support_period']}\"\n",
    "    ))\n",
    "\n",
    "    process_and_chunk(offline_courses, lambda course: (\n",
    "        f\"ê°•ì˜ ì œëª©: {course['course_title']}\\n\"\n",
    "        f\"ê°•ì˜ ë¶„ë¥˜: {course['course_category']}\\n\"\n",
    "        f\"êµìœ¡ ì¥ì†Œ: {course['education_location']}\\n\"\n",
    "        f\"êµìœ¡ ëª©í‘œ: {course['educational_goals']}\\n\"\n",
    "        f\"ìˆ˜ê°• ëŒ€ìƒ: {course['target_audience']}\\n\"\n",
    "        f\"ì§€ì› ê¸°ê°„: {course['support_period']}\\n\"\n",
    "        f\"ìˆ˜ê°• ê¸°ê°„: {course['class_period']}\\n\"\n",
    "        f\"ì§€ì› ë°©ë²•: {course['application_method']}\\n\"\n",
    "        f\"ì„ ë°œ ì ˆì°¨: {course['selection_criteria']}\\n\"\n",
    "        f\"ì´ìˆ˜ ê¸°ì¤€: {course['completion_criteria']}\"\n",
    "    ))\n",
    "\n",
    "    process_and_chunk(notice_info, lambda notice: (\n",
    "        f\"ê³µì§€ ì œëª©: {notice['notice_title']}\\n\"\n",
    "        f\"ê³µì§€ ë‚ ì§œ: {notice['notice_date']}\\n\"\n",
    "        f\"ê³µì§€ ë‚´ìš©: {notice['notice_content']}\"\n",
    "    ))\n",
    "\n",
    "    process_and_chunk(qna_info, lambda qna: (\n",
    "        f\"Q: {qna['question_title']}\\n\"\n",
    "        f\"ì§ˆë¬¸: {qna['question_content']}\\n\"\n",
    "        f\"A: {qna['answer_content']}\"\n",
    "    ))\n",
    "\n",
    "    process_and_chunk(review_info, lambda review: (\n",
    "        f\"ê°•ì˜ ì œëª©: {review['course_title']}\\n\"\n",
    "        f\"ê°•ì˜ ë³„ì : {review['rate']}\\n\"\n",
    "        f\"ì‘ì„± ë‚ ì§œ: {review['r_date']}\\n\"\n",
    "        f\"ë¦¬ë·° ë‚´ìš©: {review['content']}\\n\"\n",
    "        f\"ë¦¬ë·° ìš”ì•½: {review['summarized_content']}\"\n",
    "    ))\n",
    "\n",
    "    process_and_chunk(course_image_info, lambda image: (\n",
    "        f\"ê°•ì˜ ì œëª©: {image['course_title']}\\n\"\n",
    "        f\"ê°•ì˜ ì´ë¯¸ì§€ ì„¤ëª…: {image['image_txt']}\"\n",
    "    ))\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# í…ìŠ¤íŠ¸ë¥¼ ì¼ì •í•œ ê¸¸ì´ë¡œ ì²­í¬ ë¶„í• \n",
    "def chunk_text(text, chunk_size=500, chunk_overlap=50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "# ë²¡í„° ìŠ¤í† ì–´ ìƒì„± (FAISS ì‚¬ìš©)\n",
    "def create_vector_store(documents):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vector_store = FAISS.from_documents(documents, embeddings)\n",
    "    return vector_store\n",
    "\n",
    "# ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ê°•ì˜ë¥¼ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì±—ë´‡\n",
    "def create_chatbot(vector_store, user_prompt, system_prompt):\n",
    "    # OpenAI LLM ì„¤ì •\n",
    "    llm = OpenAI(temperature=0.2)\n",
    "\n",
    "    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "    prompt_template = \"\"\"\n",
    "    ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: {system_message}\n",
    "    ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸: {user_message}\n",
    "    ì‚¬ìš©ìì˜ ì§ˆë¬¸: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"system_message\", \"user_message\", \"question\"],\n",
    "        template=prompt_template\n",
    "    )\n",
    "\n",
    "    # PromptTemplateì„ ì‚¬ìš©í•˜ì—¬ LLMì„ ì„¤ì •\n",
    "    chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "\n",
    "    # ConversationalRetrievalChain ìƒì„±\n",
    "    retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=chat_model,\n",
    "        retriever=vector_store.as_retriever(),\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    return retrieval_chain\n",
    "\n",
    "# ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
    "def get_answer(question, retrieval_chain, chat_history):\n",
    "    try:\n",
    "        result = retrieval_chain({\"question\": question, \"chat_history\": chat_history})\n",
    "\n",
    "        # ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ë„ˆë¬´ ë§ì„ ê²½ìš° ì¼ë¶€ë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •\n",
    "        source_documents = result.get('source_documents', [])\n",
    "        max_docs = 5  # ìµœëŒ€ ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜ ì œí•œ\n",
    "        source_documents = source_documents[:max_docs]  # ìµœëŒ€ ê°œìˆ˜ë§Œí¼ ìë¥´ê¸°\n",
    "\n",
    "        answer = result.get('answer', 'ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ğŸ™')\n",
    "\n",
    "        return answer, source_documents\n",
    "    except Exception as e:\n",
    "        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì•ˆë‚´ ë©”ì‹œì§€ ë°˜í™˜\n",
    "        error_message = f\"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ğŸ˜¢\\nì˜¤ë¥˜ ë‚´ìš©: {str(e)}\"\n",
    "        return error_message, []\n",
    "\n",
    "# main í•¨ìˆ˜\n",
    "def main():\n",
    "    # ì‹œìŠ¤í…œ ë¡œì¼€ì¼ì„ 'ko_KR.UTF-8'ë¡œ ì„¤ì •í•˜ì—¬ í•œê¸€ ê¹¨ì§ì„ ë°©ì§€\n",
    "    \n",
    "    pickle_filename = \"all_documents.pkl\"\n",
    "    # 1. í”¼í´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œí•˜ê³ , ì—†ìœ¼ë©´ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ë¬¸ì„œ ìƒì„±\n",
    "    all_documents = load_from_pickle(pickle_filename)\n",
    "\n",
    "    \n",
    "    locale.setlocale(locale.LC_ALL, 'ko_KR.UTF-8')\n",
    "    if all_documents is None:\n",
    "        # ê°•ì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
    "        online_courses, offline_courses, notice_info, qna_info, review_info,course_image_info = fetch_courses()\n",
    "\n",
    "        # ê°•ì˜ ì •ë³´ë¥¼ ë¬¸ì„œë¡œ ë³€í™˜\n",
    "        documents = convert_to_documents(online_courses, offline_courses, notice_info, qna_info, review_info,course_image_info)\n",
    "\n",
    "        all_documents = []\n",
    "        all_documents.extend(documents)\n",
    "\n",
    "        save_to_pickle(all_documents, pickle_filename)\n",
    "    else:\n",
    "        documents = all_documents\n",
    "\n",
    "    # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "    vector_store = create_vector_store(documents)\n",
    "\n",
    "    # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì™€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "    system_prompt = \"\"\"\n",
    "    ë‹¹ì‹ ì€ ì‹¹ê°€ëŠ¥íŒ€ì—ì„œ ë§Œë“  í•œêµ­ì˜ ë„ì‹œ ì„œìš¸ì—ì„œ ì œê³µí•˜ëŠ” ì·¨ì—… í”„ë¡œê·¸ë¨ì¸ ì²­ë…„ì·¨ì—…ì‚¬ê´€í•™êµì˜ ì±—ë´‡ 'ì‹¹í†¡'ì…ë‹ˆë‹¤. \n",
    "    ë‹¹ì‹ ì€ ì²­ë…„ì‚¬ê´€í•™êµì˜ ê°•ì˜,ê³µì§€ì‚¬í•­ì— ê´€í•œ ì •ë³´ë¥¼ ëª¨ë‘ ê°–ê³  ìˆëŠ” ìœ ëŠ¥í•œ ê´€ë¦¬ìì…ë‹ˆë‹¤.\n",
    "    ëŒ€ë‹µí•  ë•Œ ë°˜ë“œì‹œ ì ì ˆí•œ ì´ëª¨ì§€ë¥¼ í¬í•¨í•˜ì„¸ìš”! ğŸ‰  \n",
    "    ì˜ˆë¥¼ ë“¤ë©´:  \n",
    "    \"ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ë§‘ì•„ìš”! â˜€ï¸\"  \n",
    "    \"ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”! ğŸ˜ƒ\"  \n",
    "\n",
    "    \"\"\"\n",
    "    user_prompt = \"\"\"\n",
    "    ë‹¹ì‹ ì—ê²Œ ì£¼ì–´ì§„ ì •ë³´ëŠ” ì˜¨ë¼ì¸/ì˜¤í”„ë¼ì¸ ê°•ì˜ ì •ë³´ì™€ ê°•ì˜ ë¦¬ë·°, ì§ˆë¬¸ë‹µí•˜ê¸°, ê³µì§€ì‚¬í•­ ë“± ì²­ë…„ì‚¬ê´€í•™êµì™€ ê´€ë ¨ëœ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, ê·¸ ì™¸ ì§ˆë¬¸ì—ëŠ” ë‹µë³€ì´ ì–´ë µë‹¤ê³  ë§í•˜ê±°ë‚˜ ì •í™•í•œ ì§ˆë¬¸ì„ ìš”ì²­í•˜ì„¸ìš”.\n",
    "    ë‹¹ì‹ ì€ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ í• ë•Œ ë‹¤ìŒì˜ ì›ì¹™ì„ ë”°ë¦…ë‹ˆë‹¤.\n",
    "    1. ì˜¤íƒˆì ë° ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ìˆ˜ì •\n",
    "    - ì˜ëª»ëœ ë¬¸ë²•, ì˜ë¯¸ ì—†ëŠ” íŠ¹ìˆ˜ë¬¸ì, ê¹¨ì§„ ë¬¸ì(ì˜ˆ: @, #, %) ë“±ì„ ì œê±° í˜¹ì€ ìˆ˜ì •í•©ë‹ˆë‹¤.\n",
    "    2. ê°€ë…ì„± í–¥ìƒ\n",
    "    - ë¬¸ì¥ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ê³ , ê´€ë ¨ëœ ë‚´ìš©ì„ ë¬¶ì–´ì„œ ë‹¨ë½ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
    "    3. ëª©ì°¨ ë° ì„¹ì…˜ ì •ë¦¬\n",
    "    - ì œëª©ê³¼ ë³¸ë¬¸ì„ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•©ë‹ˆë‹¤.\n",
    "    4. ìˆ˜ì—… ê´€ë ¨ ë‹µë³€ì‹œ ì˜ˆì‹œë“¤ê¸°\n",
    "    - ìˆ˜ì—…ê³¼ ê´€ë ¨í•œ ì§ˆë¬¸ì´ ë“¤ì–´ì™”ì„ì‹œ, ë‹¹ì‹ ì´ ê°€ì§€ê³  ìˆëŠ” ì •ë³´ë¥¼ ìµœëŒ€í•œ í™œìš©í•œ ì˜ˆì‹œë¥¼ ê°™ì´ ë“¤ì–´ì£¼ì„¸ìš”.\n",
    "    5. ë‹¤ë¥¸ ë‚´ìš© ì°¸ê³ í•˜ë¼ê³  ë§í•˜ì§€ ì•Šê¸°.\n",
    "    - ë‹¹ì‹  ì™¸ì— ë‹¤ë¥¸ ë‹´ë‹¹ìëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # ì±—ë´‡ ìƒì„±\n",
    "    retrieval_chain = create_chatbot(vector_store, user_prompt, system_prompt)\n",
    "\n",
    "    # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”\n",
    "    chat_history = []\n",
    "\n",
    "    # Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±\n",
    "    def chat_interface(user_input):\n",
    "        try:\n",
    "            # ë‹µë³€ ìƒì„±\n",
    "            answer, _ = get_answer(user_input, retrieval_chain, chat_history)\n",
    "\n",
    "            # ëŒ€í™” ê¸°ë¡ì— (ì‚¬ìš©ì ì§ˆë¬¸, ì±—ë´‡ ë‹µë³€) ì¶”ê°€\n",
    "            chat_history.append((user_input, answer))\n",
    "\n",
    "            # ì±—ë´‡ì˜ ì‘ë‹µì„ (ì§ˆë¬¸, ì‘ë‹µ) í˜•íƒœë¡œ ë°˜í™˜\n",
    "            return [(user_input, answer)]\n",
    "        except Exception as e:\n",
    "            return [(user_input, f\"ë” ëª…í™•í•˜ê²Œ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”ğŸ˜ƒ\")]\n",
    "\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"\"\"\n",
    "        <div style=\"text-align: center;\">\n",
    "            <img src=\"https://sesac.seoul.kr/static/common/images/www/common/logo.png\" alt=\"logo\" style=\"display: inline; width: 25px;\"/>\n",
    "            <h1 style=\"display:inline; color: #6FC274;\">SESAC</h1>\n",
    "        </div>\n",
    "        \"\"\")\n",
    "        chatbot = gr.Chatbot()\n",
    "        user_input = gr.Textbox(label=\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”\", placeholder=\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”\", interactive=True )\n",
    "        user_input.submit(chat_interface, inputs=user_input, outputs=chatbot)\n",
    "\n",
    "    demo.launch()\n",
    "\n",
    "# í”„ë¡œê·¸ë¨ ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-hGK6LnUe-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
